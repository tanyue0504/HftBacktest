# 数据推送性能
## 内存优化
1. 全量读取数据可能很大（booktick一个月可达80G），但在数据无序的情况下是必要的
2. 如果数据已经按时间戳排列，那么使用chunksize并不会降低性能，甚至可能提升
因为groupby涉及排序操作，时间复杂度是O(nlogn)
拆分为k个小任务m, k * m = n
则时间复杂度O(k * mlogm) = O(nlogm) -> O(n)
从这个意义上讲chunksize越小越好
实际上如果用更底层的语言写，顺序读然后检查每行的时间戳是否与上一行不一样，则无需用groupby，时间复杂度就是O(n)
## chunksize割裂问题
1. 当chunksize过小而每个时间戳数据过多会导致一个时间戳跨越多个chunk
2. 为了解决时间戳割裂问题，迭代时检查时间戳是否与前一个数据相同，相同则concat两批数据
3. chunksize越小，触发concat的概率越高，性能损失越大
## 性能测试报告
1. 从zipped csv一次性读2000万行数据耗时5.2s，排序耗时0.12s，推送耗时36.5s，总耗时42s
2. 从zipped csv分块100w行，不检查数据割裂，读取+推送总耗时41s，
3. 从zipped csv分块100w行，检查并处理数据割裂，读取+推送总耗时41s
4. 从parquet分块100w行，检查并处理数据割裂，读取+推送总耗时36s
5. 从parquet分块10w行，检查并处理数据割裂，读取+推送总耗时36
6. 从parquet分块1w行，检查并处理数据割裂，读取+推送总耗时37s
7. 从parquet分块1k行，检查并处理数据割裂，读取+推送总耗时43s
8. 条件1 取消Data组装 直接yield None，推送耗时下降到32.5s表明核心耗时还是在groupby对数据的处理上
## 结论
1. zipped csv vs. parquet -> parquet稍微复杂一些但是速度更快
2. 检查数据割裂耗时可忽略，因此都检查吧
3. 并非越小越好，越小数据割裂的影响就显现出来了
在已经证明全量 vs. chunk几乎相同的情况下，只要内存还够就把chunksize尽量设大一些，推荐100w行

# 空推数据测试
1. 2000w行booktick+trades大约BTCUSDT一天的数据量一分钟以内完成空推
2. 加上全市场数据，加上策略逻辑，性能应该在几分钟应该就能完成一天的回测